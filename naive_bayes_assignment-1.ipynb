{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an overview of Bayes’ theorem and its applications, along with a step-by-step solution for your Naive Bayes classification assignment.\n",
    "\n",
    "Q1. What is Bayes’ theorem?\n",
    "\n",
    "Bayes’ theorem is a fundamental concept in probability theory and statistics that describes the probability of an event based on prior knowledge of related events. It provides a way to update the probability of a hypothesis in light of new evidence.\n",
    "\n",
    "Q2. What is the formula for Bayes’ theorem?\n",
    "\n",
    "The formula for Bayes’ theorem is:\n",
    "\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "\t•\t P(A|B) : The posterior probability of event  A  given  B .\n",
    "\t•\t P(B|A) : The likelihood of observing  B  given that  A  is true.\n",
    "\t•\t P(A) : The prior probability of  A .\n",
    "\t•\t P(B) : The probability of  B  (often called the “evidence”).\n",
    "\n",
    "￼\n",
    "\n",
    "Where:\n",
    "\n",
    "\t•\t￼: The posterior probability of event ￼ given ￼.\n",
    "\t•\t￼: The likelihood of observing ￼ given that ￼ is true.\n",
    "\t•\t￼: The prior probability of ￼.\n",
    "\t•\t￼: The probability of ￼ (often called the “evidence”).\n",
    "\n",
    "Q3. How is Bayes’ theorem used in practice?\n",
    "\n",
    "Bayes’ theorem is widely used in various fields, including:\n",
    "\n",
    "\t•\tMedical diagnosis: Estimating the probability of a disease given symptoms.\n",
    "\t•\tSpam filtering: Determining whether an email is spam based on words it contains.\n",
    "\t•\tMachine learning: In classifiers like Naive Bayes, which predict class membership.\n",
    "\t•\tRisk assessment: Calculating the likelihood of risk based on historical data.\n",
    "\n",
    "Q4. What is the relationship between Bayes’ theorem and conditional probability?\n",
    "\n",
    "Bayes’ theorem is essentially a way to calculate conditional probability. It provides a method for updating the probability of an event given new information, turning prior probability into posterior probability.\n",
    "\n",
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "Naive Bayes classifiers come in several types, mainly distinguished by how they handle the features:\n",
    "\n",
    "\t1.\tGaussian Naive Bayes: Used when features are continuous and assumed to follow a normal (Gaussian) distribution.\n",
    "\t2.\tMultinomial Naive Bayes: Used for discrete count features (e.g., word frequencies in text classification).\n",
    "\t3.\tBernoulli Naive Bayes: Used for binary/Boolean features (e.g., presence or absence of words).\n",
    "\n",
    "The choice depends on the nature of your features. Gaussian Naive Bayes is suitable for continuous data, while Multinomial and Bernoulli are preferred for discrete or binary data.\n",
    "\n",
    "Q6. Assignment Solution: Naive Bayes Classification\n",
    "\n",
    "Given the table for features ￼ and ￼, the frequency of each feature value for each class, and equal prior probabilities, let’s use Naive Bayes to predict the class for a new instance with ￼ and ￼.\n",
    "\n",
    "\t1.\tCalculate prior probabilities:\n",
    "Since both classes ￼ and ￼ have equal prior probabilities:\n",
    "\n",
    "\n",
    "P(A) = P(B) = 0.5\n",
    "\n",
    "￼\n",
    "\t2.\tCalculate likelihoods for each feature given each class:\n",
    "\t\t•\tFor  X1 = 3 :\n",
    "\t•\t P(X1 = 3 | A) = \\frac{4}{10} = 0.4 \n",
    "\t•\t P(X1 = 3 | B) = \\frac{1}{5} = 0.2 \n",
    "\t•\tFor  X2 = 4 :\n",
    "\t•\t P(X2 = 4 | A) = \\frac{3}{10} = 0.3 \n",
    "\t•\t P(X2 = 4 | B) = \\frac{3}{5} = 0.6 \n",
    "\t\n",
    "\t3.\tCalculate the posterior probabilities:\n",
    "\t•\tFor class  A :\n",
    "\n",
    "P(A | X1 = 3, X2 = 4) \\propto P(X1 = 3 | A) \\cdot P(X2 = 4 | A) \\cdot P(A)\n",
    "\n",
    "\n",
    "= 0.4 \\cdot 0.3 \\cdot 0.5 = 0.06\n",
    "\n",
    "\t•\tFor class  B :\n",
    "\n",
    "P(B | X1 = 3, X2 = 4) \\propto P(X1 = 3 | B) \\cdot P(X2 = 4 | B) \\cdot P(B)\n",
    "\n",
    "\n",
    "= 0.2 \\cdot 0.6 \\cdot 0.5 = 0.06\n",
    "\n",
    "￼\n",
    "\t4.\tDecision:\n",
    "Since both probabilities are equal, the Naive Bayes classifier would not have a preference based on probability alone. However, a common tie-breaking rule is to choose the class with the higher prior probability or assign it randomly if prior probabilities are equal. In this case, with equal priors, you could assign either ￼ or ￼, or opt for further feature engineering or additional features if the choice is ambiguous.\n",
    "\n",
    "Thus, Naive Bayes does not decisively predict a class in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
