{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go through each question and the steps for the Naive Bayes assignment.\n",
    "\n",
    "Q1. Probability that an employee is a smoker given they use the health insurance plan\n",
    "Let:\n",
    "\n",
    "\t•\t H : An employee uses the health insurance plan.\n",
    "\t•\t S : An employee is a smoker.\n",
    "\n",
    "We know:\n",
    "\n",
    "\t•\t P(H) = 0.7  (70% of employees use the health insurance).\n",
    "\t•\t P(S | H) = 0.4  (40% of employees who use the health insurance are smokers).\n",
    "\n",
    "We want  P(S | H) , the probability that an employee is a smoker given they use the health insurance plan. We are already given this directly:\n",
    "\n",
    "\n",
    "P(S | H) = 0.4\n",
    "\n",
    "\n",
    "So, the probability that an employee is a smoker given that they use the health insurance plan is 0.4 or 40%.\n",
    "\n",
    "\n",
    "Q2. Difference between Bernoulli Naive Bayes and Multinomial Naive Bayes\n",
    "\n",
    "\t1.\tBernoulli Naive Bayes:\n",
    "\t•\tUsed for binary or boolean features (e.g., 0 or 1, representing the presence or absence of a term in text classification).\n",
    "\t•\tIt considers each feature’s presence/absence independently in the probability calculation, which is well-suited for binary data.\n",
    "\t•\tWorks best for data where features indicate binary conditions.\n",
    "\t2.\tMultinomial Naive Bayes:\n",
    "\t•\tBest suited for discrete count data (e.g., word counts in documents).\n",
    "\t•\tIt uses feature frequencies directly rather than binary indicators.\n",
    "\t•\tWorks well in text classification tasks where the importance of repeated terms should influence classification (e.g., document word counts).\n",
    "\n",
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Bernoulli Naive Bayes does not inherently handle missing values. In cases where there are missing values:\n",
    "\n",
    "\t•\tThey are typically replaced with binary indicators (e.g., 0 for absence) or imputed using methods like mean/median imputation.\n",
    "\t•\tAlternatively, missing values can be treated with a “missing” category, indicating that no information is available for that feature.\n",
    "\n",
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Scikit-learn’s GaussianNB class, for example, handles multi-class classification by applying the Bayes theorem independently for each class. The class with the highest posterior probability is selected as the predicted class.\n",
    "\n",
    "Q5. Assignment: Implementing Naive Bayes Classifiers on the Spambase Data Set\n",
    "\n",
    "Step-by-Step Solution:\n",
    "\n",
    "\t1.\tData Preparation:\n",
    "\t•\tDownload the “Spambase Data Set” from the UCI Machine Learning Repository.\n",
    "\t•\tLoad the dataset into a DataFrame in Python.\n",
    "\t•\tSplit the dataset into features (X) and target (y), where the target indicates whether an email is spam.\n",
    "\t2.\tImplementation:\n",
    "\t•\tImport necessary libraries (pandas, sklearn for Naive Bayes and metrics, cross_val_score for cross-validation).\n",
    "\t•\tInitialize three classifiers with default hyperparameters: BernoulliNB, MultinomialNB, and GaussianNB.\n",
    "\t•\tUse 10-fold cross-validation for each classifier to evaluate performance metrics (Accuracy, Precision, Recall, F1 score).\n",
    "\t3.\tResults Calculation:\n",
    "\t•\tUse cross_val_score to obtain metrics for each classifier.\n",
    "\t•\tCalculate average values of Accuracy, Precision, Recall, and F1 score for each classifier across folds.\n",
    "\t4.\tDiscussion:\n",
    "\t•\tPerformance Analysis: Compare the performance metrics across classifiers.\n",
    "\t•\tBest Performing Model: Typically, Multinomial Naive Bayes performs better on text data with word frequencies, as it considers counts, while Bernoulli focuses on binary presence/absence.\n",
    "\t•\tLimitations Observed: Naive Bayes assumes feature independence, which may not hold in real data. Multinomial may fail on data where word frequency doesn’t correlate with spam likelihood.\n",
    "\t5.\tConclusion:\n",
    "\t•\tSummary of Findings: Multinomial Naive Bayes may likely perform the best due to its natural fit with the word frequency data in spam classification.\n",
    "\t•\tSuggestions for Future Work: Try tuning hyperparameters, exploring alternative models like SVMs or ensemble methods, and using additional feature engineering techniques to improve classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
